<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Predmachlearn-010 : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Predmachlearn-010</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/StevenFupc/predmachlearn-010">View on GitHub</a>

          <h1 id="project_title">Predmachlearn-010</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/StevenFupc/predmachlearn-010/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/StevenFupc/predmachlearn-010/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>&lt;!DOCTYPE html&gt;</p>

<p></p>

<p></p>

<p>

</p>

<p></p>Practical Machine Learning - Activity Quality Prediction



<p>

</p>



code{white-space: pre;}

<p></p>




  pre:not([class]) {
    background-color: white;
  }




<p></p>

<p></p>


.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}


<div>


<div id="header">
<h1>
<a id="practical-machine-learning---activity-quality-prediction" class="anchor" href="#practical-machine-learning---activity-quality-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Practical Machine Learning - Activity Quality Prediction</h1>
</div>

<div id="introduction">
<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.</p>
</div>

<div id="methods">
<h2>
<a id="methods" class="anchor" href="#methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Methods</h2>
<div id="getting-data">
<h3>
<a id="getting-data" class="anchor" href="#getting-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting data</h3>
<p>The data for this project come from this source: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). The data were recorded from accelerometers. We downloaded <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">pml-training.csv</a> and <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">pml-testing.csv</a> datasets on 26 Jan 2015.</p>
<pre><code>set.seed(111)
library(caret, quietly=T)</code></pre>
<pre><code>## Warning: package 'caret' was built under R version 3.1.2</code></pre>
<pre><code>## Warning: package 'ggplot2' was built under R version 3.1.2</code></pre>
<pre><code>setwd("C:\\Users\\fupc\\Desktop\\c8\\proj")
data &lt;- read.csv("pml-training.csv", na.strings="NA", header=T)
data &lt;- data[sample(nrow(data)),]
inTrain &lt;- createDataPartition(data$X, p = 0.7)[[1]]
train &lt;- data[inTrain,]
valid &lt;- data[-inTrain,]
test &lt;- read.csv("pml-testing.csv", na.strings="NA", header=T)</code></pre>
</div>

<div id="cleaning-data">
<h3>
<a id="cleaning-data" class="anchor" href="#cleaning-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning data</h3>
<p>This step is to remove columns that are mostly NAs.</p>
<pre><code>idx2remove &lt;- function(df){
  idx &lt;- c()
  for (i in 1:dim(df)[2]){
    if (sum(is.na(df[,i]))/dim(df)[1] &gt; 0.1) idx &lt;- c(idx, i)
  }
  return(idx)
}
col.to.remove &lt;- unique(c(idx2remove(train), idx2remove(valid), idx2remove(test)))

trainC &lt;- train$classe
train &lt;- train[,-c(col.to.remove)]

validC &lt;- valid$classe
valid &lt;- valid[,-c(col.to.remove)]

testC &lt;- test$problem_id
test &lt;- test[,-c(col.to.remove)]
train &lt;- train[,sapply(train, is.numeric)]
valid &lt;- valid[,sapply(valid, is.numeric)]
test &lt;- test[,sapply(test, is.numeric)]</code></pre>
<p>This step is to remove columns with high correlation (correlation over 0.9).</p>
<pre><code>num.idx &lt;- which(sapply(train, class) == "numeric")
descrCorr &lt;- abs(cor(train[,num.idx]))
highCorr &lt;- findCorrelation(descrCorr, 0.9)
highCorrCol &lt;- dimnames(descrCorr[, highCorr])[[2]]
train &lt;- train[, !names(train) %in% c(highCorrCol, "X", "cvtd_timestamp", "raw_timestamp_part_1",  "raw_timestamp_part_2",  "num_window", "classe")]
valid &lt;-valid[, !names(valid) %in% c(highCorrCol, "X", "cvtd_timestamp", "raw_timestamp_part_1",  "raw_timestamp_part_2",   "num_window", "classe")]
test &lt;- test[, !names(test) %in% c(highCorrCol, "X", "cvtd_timestamp", "raw_timestamp_part_1",  "raw_timestamp_part_2", "num_window", "problem_id")]
train &lt;- sapply(train, as.numeric)
valid &lt;- sapply(valid, as.numeric)
test &lt;- as.matrix(sapply(test, as.numeric))</code></pre>
</div>

<div id="statistical-modeling">
<h3>
<a id="statistical-modeling" class="anchor" href="#statistical-modeling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Statistical Modeling</h3>
<p>As input data is high-dimensional, Random Forest from the <strong>randomForest</strong> package is chosen to building a predictive model. We built the model for the outcome variable <em>classe</em> using 49 predictive variables.</p>
<pre><code>library(randomForest, quietly=T)</code></pre>
<pre><code>## Warning: package 'randomForest' was built under R version 3.1.2</code></pre>
<pre><code>## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>fitRF &lt;- randomForest(x=train, y=trainC, importance=T, ntree=317)
fitRF</code></pre>
<pre><code>## 
## Call:
##  randomForest(x = train, y = trainC, ntree = 317, importance = T) 
##                Type of random forest: classification
##                      Number of trees: 317
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.52%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 3900    2    0    0    0 0.0005125577
## B   12 2628    6    0    0 0.0068027211
## C    0   17 2376    6    0 0.0095873281
## D    0    0   21 2253    2 0.0101054482
## E    0    0    1    4 2510 0.0019880716</code></pre>
<p>Here we can see 5 the most important variables</p>
<pre><code>varImpPlot(fitRF, n.var=5, type=1, main="Fig. 1. Variable Importance")</code></pre>
<p><img title alt width="768"> With number of trees of 317 the Random Forest was successfully trained and the estimated OOB error rate (in sample error) for the training set was 0.52%. We don’t need to do cross-validation as random forest uses bootstrap samples from the train set.</p>
</div>

<div id="results">
<h3>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h3>
<p>On the fig. 1 we can see the most important variables for the predicting model: yaw_belt, roll_belt, pitch_belt.</p>
<p>Let’s plot the three variables. On the figures 2-4 we can see the relationship between those variables. We color grouped the points based on the 5 levels of activities quality. The figures show rather complicated patterns which cannot be linearly separated, thus, we used random forest.</p>
<pre><code>par(mfrow=c(2,2))
valid &lt;- data.frame(valid)
plot(valid$yaw_belt, valid$roll_belt,
     xlab="Yaw belt", 
     ylab="Roll belt", 
     main="Fig. 2",col=as.numeric(validC), pch=19, cex=.5)

plot(valid$yaw_belt,valid$pitch_belt,
     xlab="Yaw belt", 
     ylab="Pitch belt", 
     main="Fig. 3",col=as.numeric(validC), pch=19, cex=.5)

plot(valid$roll_belt,valid$pitch_belt,
     xlab="Roll belt", 
     ylab="Pitch belt", 
     main="Fig. 4",col=as.numeric(validC), pch=19, cex=.5)
plot.new()
legend("center", legend=levels(validC), cex=2, text.col=seq_along(levels(validC)))</code></pre>
<p><img title alt width="1056"> We tested out model on validation set and observed a confusion matrix for the validation set.</p>
<pre><code>predRF &lt;- predict(fitRF, valid)
confusionMatrix(predRF, validC)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1678    8    0    0    0
##          B    0 1142    3    0    0
##          C    0    1 1019    7    1
##          D    0    0    1  933    1
##          E    0    0    0    0 1090
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9963          
##                  95% CI : (0.9943, 0.9977)
##     No Information Rate : 0.2852          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9953          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9922   0.9961   0.9926   0.9982
## Specificity            0.9981   0.9994   0.9981   0.9996   1.0000
## Pos Pred Value         0.9953   0.9974   0.9912   0.9979   1.0000
## Neg Pred Value         1.0000   0.9981   0.9992   0.9986   0.9996
## Prevalence             0.2852   0.1956   0.1739   0.1598   0.1856
## Detection Rate         0.2852   0.1941   0.1732   0.1586   0.1852
## Detection Prevalence   0.2865   0.1946   0.1747   0.1589   0.1852
## Balanced Accuracy      0.9990   0.9958   0.9971   0.9961   0.9991</code></pre>
<p>Out of sample error is about 0.37%. Finally we tested our model on the 20 test cases available in the test data. We got 100% prediction accuracy.</p>
<pre><code>predRFT &lt;- predict(fitRF, test)
answer &lt;- as.character(predRFT)
pml_write_files &lt;- function(x){
  n &lt;- length(x)
  for(i in 1:n){
    filename &lt;- paste0("problem_id_",i,".txt")
    write.table(x[i],file &lt;- filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answer)</code></pre>
</div>

<div id="conclusions">
<h3>
<a id="conclusions" class="anchor" href="#conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions</h3>
<p>In this paper we proposed a predictive model based on Random Forest. This model provides high accuracy in identifying of activity quality.</p>
</div>

<p></p>
</div>

<p></p>
</div>







<p>
</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Predmachlearn-010 maintained by <a href="https://github.com/StevenFupc">StevenFupc</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
